#fit1.RMSE = sqrt(mean((fit1.predict-y.test)^2))
fit1.logistPred = logist(fit1.predict)
fit1.logistPred.RMSE = sqrt(mean((fit1.logistPred-y.test)^2))
# Elastic Net, Alpha = .5
fit2<- cv.glmnet(y = y.train, x= x.train, alpha=0.5, family='binomial', type='mse')
best.lambda = fit2$lambda.min
fit2.predict = predict(fit2, s= best.lambda, newx = x.test)
#fit2.RMSE = sqrt(mean((fit2.predict-y.test)^2))
fit2.logistPred = logist(fit2.predict)
fit2.logistPred.RMSE = sqrt(mean((fit2.logistPred-y.test)^2))
# Elastic Net, Alpha = .25
fit3<- cv.glmnet(y = y.train, x= x.train, alpha=0.25, family='binomial', type='mse')
best.lambda = fit3$lambda.min
fit3.predict = predict(fit3, s= best.lambda, newx = x.test)
#fit3.RMSE = sqrt(mean((fit3.predict-y.test)^2))
fit3.logistPred = logist(fit3.predict)
fit3.logistPred.RMSE = sqrt(mean((fit3.logistPred-y.test)^2))
# Fit4 not published in paper -- so not doing it
#fit4<- cv.glmnet(y = Y, x= Xfull, alpha=0, family='binomial', type='mse')
## Skipping FindIt since documentation changed (per)
# Bayesian GLM -- Revisit -- probably not working right
#Not sure if I should directly predict via linear estiamtion, or use a logistic regression
# Prediction Function Doesn't really work for this
#install.packages("arm")
library(arm)
fit6<- bayesglm(y.train~x.train-1, family=binomial(link=logit))
#fit6.predict = x.test%*%fit6$coefficients
fit6.predict2 = logist(x.test%*%fit6$coefficients)
#fit6.1 = bayesglm(y~x-1, family=binomial(link=logit))
#fit6.RMSE = sqrt(mean(resid(fit6.1)^2))
fit6.logistPred.RMSE = sqrt(mean((fit6.predict2-y.test)^2))
# Fit 7 = Boosted Trees is not published ### SKipping
# Fit 8 = BART
library(BayesTree)
fit8<- bart(x.train=x.train, y.train=factor(y.train), x.test=x.test, ndpost=1000, nskip=500, usequants=T)
fit8.pred<- pnorm(apply(fit8$yhat.test, 2, mean))
fit8.rmse = sqrt(mean((fit8.pred-y.train)^2))
# Fit 9  = RandomForest
library(randomForest)
fit9<- randomForest(y = factor(y.train), x = x.train)
X.test.forest = X.test
`colnames<-`(X.test,colnames(x.train))
#Only works for X.test??
fit9.pred.raw = predict(fit9,newdata = X.test.forest,type = "prob" )
fit9.pred = fit9.pred.raw[,2]
fit9.rmse = sqrt(mean((fit9.pred-y.test)^2))
# Fit 10 = Skipped in Paper and SLF_round2 code
# Fit 11 = KRLS
library(KRLS)
fit11<- krls(X = x.train[,-1], y = y.train, derivative=F)
fit11.predict = predict(fit11,newdata = x.test[,-1])$fit
fit11.rmse = sqrt(mean((fit11.predict-y.test)^2))
# Fit 12 = SVM-SMO
library(rJava)
.jinit(parameters="-Xmx4g")
library(RWeka)
subset.index = (1:length(y))[-test.indexes]
fit12 <- SMO(y ~ ., data = data.frame(Y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
#fit12 <- SMO(Y ~ ., data = data.frame(Y=factor(Y),Xfull), control = Weka_control(M = TRUE ) , subset = ((1:1074)[-test.indexes]))
fit12.predict =predict(fit12, newdata= data.frame(x[test.indexes,]), type="probability" )[,2]
fit12.RMSE = sqrt(mean((fit12.predict-y.test)^2))
# Fit 13 = Simple Mean
fit13.predict = mean(y.train)
fit13.RSME= sqrt(mean((fit13.predict-y.test)^2))
models = c("Lasso", "Elastic Net (a = .5)","Elastic Net (a = .25)", "Bayesian GLM", "BART", "Random Forest", "KRLS", "SVM-SMO", "Simple Average")
RMSE.all = c(fit1.logistPred.RMSE,fit2.logistPred.RMSE,fit3.logistPred.RMSE,fit6.logistPred.RMSE,fit8.rmse,fit9.rmse,fit11.rmse,fit12.RMSE, fit13.RSME)
return(data.frame(models,RMSE.all))
}
data.frame(models,RMSE.all)
test.indexes = sample(1074,107)
RMSEforModel = function(x,y){
# NOTE: THIS IS NOT DOING CROSS-VALIDATION right now (for most models)
test.indexes = sample(1074,107)
x.train = x[-test.indexes]
x.test = x[test.indexes]
y.train = y[-test.indexes]
y.test = y[test.indexes]
library(glmnet)
##Now predict them
logist<- function(x){
ff<- 1/(1 + exp(-x))
return(ff)
}
# Lasso
fit1<- cv.glmnet(x = x.train, y = y.train, alpha=1, family='binomial', type='mse')
best.lambda = fit1$lambda.min
fit1.predict = predict(fit1, s= best.lambda, newx = x.test)
#fit1.RMSE = sqrt(mean((fit1.predict-y.test)^2))
fit1.logistPred = logist(fit1.predict)
fit1.logistPred.RMSE = sqrt(mean((fit1.logistPred-y.test)^2))
# Elastic Net, Alpha = .5
fit2<- cv.glmnet(y = y.train, x= x.train, alpha=0.5, family='binomial', type='mse')
best.lambda = fit2$lambda.min
fit2.predict = predict(fit2, s= best.lambda, newx = x.test)
#fit2.RMSE = sqrt(mean((fit2.predict-y.test)^2))
fit2.logistPred = logist(fit2.predict)
fit2.logistPred.RMSE = sqrt(mean((fit2.logistPred-y.test)^2))
# Elastic Net, Alpha = .25
fit3<- cv.glmnet(y = y.train, x= x.train, alpha=0.25, family='binomial', type='mse')
best.lambda = fit3$lambda.min
fit3.predict = predict(fit3, s= best.lambda, newx = x.test)
#fit3.RMSE = sqrt(mean((fit3.predict-y.test)^2))
fit3.logistPred = logist(fit3.predict)
fit3.logistPred.RMSE = sqrt(mean((fit3.logistPred-y.test)^2))
# Fit4 not published in paper -- so not doing it
#fit4<- cv.glmnet(y = Y, x= Xfull, alpha=0, family='binomial', type='mse')
## Skipping FindIt since documentation changed (per)
# Bayesian GLM -- Revisit -- probably not working right
#Not sure if I should directly predict via linear estiamtion, or use a logistic regression
# Prediction Function Doesn't really work for this
#install.packages("arm")
library(arm)
fit6<- bayesglm(y.train~x.train-1, family=binomial(link=logit))
#fit6.predict = x.test%*%fit6$coefficients
fit6.predict2 = logist(x.test%*%fit6$coefficients)
#fit6.1 = bayesglm(y~x-1, family=binomial(link=logit))
#fit6.RMSE = sqrt(mean(resid(fit6.1)^2))
fit6.logistPred.RMSE = sqrt(mean((fit6.predict2-y.test)^2))
# Fit 7 = Boosted Trees is not published ### SKipping
# Fit 8 = BART
library(BayesTree)
fit8<- bart(x.train=x.train, y.train=factor(y.train), x.test=x.test, ndpost=1000, nskip=500, usequants=T)
fit8.pred<- pnorm(apply(fit8$yhat.test, 2, mean))
fit8.rmse = sqrt(mean((fit8.pred-y.train)^2))
# Fit 9  = RandomForest
library(randomForest)
fit9<- randomForest(y = factor(y.train), x = x.train)
X.test.forest = X.test
`colnames<-`(X.test,colnames(x.train))
#Only works for X.test??
fit9.pred.raw = predict(fit9,newdata = X.test.forest,type = "prob" )
fit9.pred = fit9.pred.raw[,2]
fit9.rmse = sqrt(mean((fit9.pred-y.test)^2))
# Fit 10 = Skipped in Paper and SLF_round2 code
# Fit 11 = KRLS
library(KRLS)
fit11<- krls(X = x.train[,-1], y = y.train, derivative=F)
fit11.predict = predict(fit11,newdata = x.test[,-1])$fit
fit11.rmse = sqrt(mean((fit11.predict-y.test)^2))
# Fit 12 = SVM-SMO
library(rJava)
.jinit(parameters="-Xmx4g")
library(RWeka)
subset.index = (1:length(y))[-test.indexes]
fit12 <- SMO(y ~ ., data = data.frame(Y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
#fit12 <- SMO(Y ~ ., data = data.frame(Y=factor(Y),Xfull), control = Weka_control(M = TRUE ) , subset = ((1:1074)[-test.indexes]))
fit12.predict =predict(fit12, newdata= data.frame(x[test.indexes,]), type="probability" )[,2]
fit12.RMSE = sqrt(mean((fit12.predict-y.test)^2))
# Fit 13 = Simple Mean
fit13.predict = mean(y.train)
fit13.RSME= sqrt(mean((fit13.predict-y.test)^2))
models = c("Lasso", "Elastic Net (a = .5)","Elastic Net (a = .25)", "Bayesian GLM", "BART", "Random Forest", "KRLS", "SVM-SMO", "Simple Average")
RMSE.all = c(fit1.logistPred.RMSE,fit2.logistPred.RMSE,fit3.logistPred.RMSE,fit6.logistPred.RMSE,fit8.rmse,fit9.rmse,fit11.rmse,fit12.RMSE, fit13.RSME)
return(data.frame(models,RMSE.all))
}
dem<- ifelse(svdat$pid3l=='Dem', 1, 0)  #line 366-369 of rep code
dem[which(is.na(dem))]<- 0
rep<- ifelse(svdat$pid3l=='Rep', 1, 0)
rep[which(is.na(rep))]<- 0
cons<- ifelse(svdat$ideo3<3, 1, 0) #line 230-231 of rep code
lib<- ifelse(svdat$ideo3==4|svdat$ideo3==5, 1, 0)
lib[which(is.na(lib))]<- 0 #line 370-371 of rep code
cons[which(is.na(cons))]<- 0
############ Defining treats
type.mat<- matrix(0, nrow = 1074, ncol=7)
colnames(type.mat)<- sort(unique(as.character(svdat$cond.type)))
for(z in 1:nrow(type.mat)){
type.mat[z,which(colnames(type.mat)==svdat$cond.type[z])]<- 1
}
type.mat.final<- type.mat[,-1]
types<- sort(unique(as.character(svdat$cond.type)))
type.num<- match(svdat$cond.type, types)
number<- c('control', '$20 million', '$50 thousand')
amount.num<- match(svdat$cond.money, number)
request<- c('control', 'requested', 'secured', 'will request')
stage.num<- match(svdat$cond.stage, request)
party<- c('control', 'a Republican', 'a Democrat')
party.num<- match(svdat$cond.party, party)
along<- c('control', 'alone', 'w/ Rep', 'w/ Dem')
along.num<- match(svdat$cond.alongWith, along)
num.mat<- matrix(0, nrow=1074, ncol=3)
colnames(num.mat)<- number
for(z in 1:nrow(num.mat)){
num.mat[z,which(colnames(num.mat)==svdat$cond.money[z])]<- 1
}
num.mat.final<- num.mat[,-1]
stage.mat<- matrix(0, nrow=1074, ncol=4)
colnames(stage.mat)<- request
for(z in 1:nrow(stage.mat)){
stage.mat[z,which(colnames(stage.mat)==svdat$cond.stage[z])]<- 1
}
stage.mat.final<- stage.mat[,-1]
party.mat<- matrix(0, nrow=1074, ncol=3)
colnames(party.mat)<- party
for(z in 1:nrow(party.mat)){
party.mat[z, which(colnames(party.mat)==svdat$cond.party[z])]<- 1
}
party.mat.final<- party.mat[,-1]
along.mat<- matrix(0, nrow=1074, ncol=4)
colnames(along.mat)<- 	along
for(z in 1:nrow(along.mat)){
along.mat[z,which(colnames(along.mat)==svdat$cond.alongWith[z])]<- 1
}
along.mat.final<- along.mat[,-1]
treats<- cbind(type.mat.final, num.mat.final[,1], stage.mat.final[,1:2],party.mat.final[,1],
along.mat.final[,1:2], type.mat.final[,1:5]*num.mat.final[,1], type.mat.final[,1:5]*stage.mat.final[,1],
type.mat.final[,1:5]*stage.mat.final[,2], type.mat.final[,1:5]*party.mat.final[,1], type.mat.final[,1:5]*along.mat.final[,1],
type.mat.final[,1:5]*along.mat.final[,2], num.mat.final[,1]*stage.mat.final[,1], num.mat.final[,1]*stage.mat.final[,2],
num.mat.final[,1]*party.mat.final[,1], num.mat.final[,1]*along.mat.final[,1], num.mat.final[,1]*along.mat.final[,2],
stage.mat.final[,1:2]*party.mat.final[,1], stage.mat.final[,1:2]*along.mat.final[,1],
stage.mat.final[,1:2]*along.mat.final[,2], party.mat.final[,1]*along.mat.final[,1], party.mat.final[,1]*along.mat.final[,2] )
treat<- treats #line 448 of rep code
### Defining the X variable
covs<- cbind(dem, rep, lib, cons) #line 373 of rep code
X <- covs #line 432 of repcode
Xfull <- model.matrix(~X*treat)
## line 391 of rep code
#Defining the Y variable
#line 432 of rep code
Y<- approve_bi<- ifelse(svdat$approval<3, 1, 0) #line 292 of rep code
RMSEforModel(X,Y)
RMSEforModel = function(x,y){
# NOTE: THIS IS NOT DOING CROSS-VALIDATION right now (for most models)
test.indexes = sample(1074,107)
x.train = x[-test.indexes]
x.test = x[test.indexes]
y.train = y[-test.indexes]
y.test = y[test.indexes]
library(glmnet)
##Now predict them
logist<- function(x){
ff<- 1/(1 + exp(-x))
return(ff)
}
# Lasso
fit1<- cv.glmnet(x = x.train, y = y.train, alpha=1, family='binomial', type='mse')
best.lambda = fit1$lambda.min
fit1.predict = predict(fit1, s= best.lambda, newx = x.test)
#fit1.RMSE = sqrt(mean((fit1.predict-y.test)^2))
fit1.logistPred = logist(fit1.predict)
fit1.logistPred.RMSE = sqrt(mean((fit1.logistPred-y.test)^2))
# Elastic Net, Alpha = .5
fit2<- cv.glmnet(y = y.train, x= x.train, alpha=0.5, family='binomial', type='mse')
best.lambda = fit2$lambda.min
fit2.predict = predict(fit2, s= best.lambda, newx = x.test)
#fit2.RMSE = sqrt(mean((fit2.predict-y.test)^2))
fit2.logistPred = logist(fit2.predict)
fit2.logistPred.RMSE = sqrt(mean((fit2.logistPred-y.test)^2))
# Elastic Net, Alpha = .25
fit3<- cv.glmnet(y = y.train, x= x.train, alpha=0.25, family='binomial', type='mse')
best.lambda = fit3$lambda.min
fit3.predict = predict(fit3, s= best.lambda, newx = x.test)
#fit3.RMSE = sqrt(mean((fit3.predict-y.test)^2))
fit3.logistPred = logist(fit3.predict)
fit3.logistPred.RMSE = sqrt(mean((fit3.logistPred-y.test)^2))
# Fit4 not published in paper -- so not doing it
#fit4<- cv.glmnet(y = Y, x= Xfull, alpha=0, family='binomial', type='mse')
## Skipping FindIt since documentation changed (per)
# Bayesian GLM -- Revisit -- probably not working right
#Not sure if I should directly predict via linear estiamtion, or use a logistic regression
# Prediction Function Doesn't really work for this
#install.packages("arm")
library(arm)
fit6<- bayesglm(y.train~x.train-1, family=binomial(link=logit))
#fit6.predict = x.test%*%fit6$coefficients
fit6.predict2 = logist(x.test%*%fit6$coefficients)
#fit6.1 = bayesglm(y~x-1, family=binomial(link=logit))
#fit6.RMSE = sqrt(mean(resid(fit6.1)^2))
fit6.logistPred.RMSE = sqrt(mean((fit6.predict2-y.test)^2))
# Fit 7 = Boosted Trees is not published ### SKipping
# Fit 8 = BART
library(BayesTree)
fit8<- bart(x.train=x.train, y.train=factor(y.train), x.test=x.test, ndpost=1000, nskip=500, usequants=T)
fit8.pred<- pnorm(apply(fit8$yhat.test, 2, mean))
fit8.rmse = sqrt(mean((fit8.pred-y.train)^2))
# Fit 9  = RandomForest
library(randomForest)
fit9<- randomForest(y = factor(y.train), x = x.train)
X.test.forest = X.test
`colnames<-`(X.test,colnames(x.train))
#Only works for X.test??
fit9.pred.raw = predict(fit9,newdata = X.test.forest,type = "prob" )
fit9.pred = fit9.pred.raw[,2]
fit9.rmse = sqrt(mean((fit9.pred-y.test)^2))
# Fit 10 = Skipped in Paper and SLF_round2 code
# Fit 11 = KRLS
library(KRLS)
fit11<- krls(X = x.train[,-1], y = y.train, derivative=F)
fit11.predict = predict(fit11,newdata = x.test[,-1])$fit
fit11.rmse = sqrt(mean((fit11.predict-y.test)^2))
# Fit 12 = SVM-SMO
library(rJava)
.jinit(parameters="-Xmx4g")
library(RWeka)
subset.index = (1:length(y))[-test.indexes]
fit12 <- SMO(y ~ ., data = data.frame(Y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
#fit12 <- SMO(Y ~ ., data = data.frame(Y=factor(Y),Xfull), control = Weka_control(M = TRUE ) , subset = ((1:1074)[-test.indexes]))
fit12.predict =predict(fit12, newdata= data.frame(x[test.indexes,]), type="probability" )[,2]
fit12.RMSE = sqrt(mean((fit12.predict-y.test)^2))
# Fit 13 = Simple Mean
fit13.predict = mean(y.train)
fit13.RSME= sqrt(mean((fit13.predict-y.test)^2))
models = c("Lasso", "Elastic Net (a = .5)","Elastic Net (a = .25)", "Bayesian GLM", "BART", "Random Forest", "KRLS", "SVM-SMO", "Simple Average")
RMSE.all = c(fit1.logistPred.RMSE,fit2.logistPred.RMSE,fit3.logistPred.RMSE,fit6.logistPred.RMSE,fit8.rmse,fit9.rmse,fit11.rmse,fit12.RMSE, fit13.RSME)
return(data.frame(models,RMSE.all))
}
RMSEforModel(X,Y)
set.seed(100) #can provide any number for seed
nall = nrow(covs) #total number of rows in data
ntrain = floor(0.7 * nall) # number of rows for train,70%
ntest = floor(0.3* nall) # number of rows for test, 30%
index = seq(1:nall)
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
train = mydata[trainIndex,]
train = covs[trainIndex,]
test = covs[test,]
testIndex = index[-train]
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
test = covs[testIndex,]
set.seed(100) #can provide any number for seed
nall = nrow(covs) #total number of rows in data
ntrain = floor(0.9 * nall) # number of rows for train,70%
ntest = floor(0.1* nall) # number of rows for test, 30%
index = seq(1:nall)
train = covs[trainIndex,]
test = covs[testIndex,]
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
View(test)
View(train)
set.seed(100) #can provide any number for seed
nall = nrow(covs) #total number of rows in data
ntrain = floor(0.9 * nall) # number of rows for train,70%
ntest = floor(0.1* nall) # number of rows for test, 30%
index = seq(1:nall)
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
train = covs[trainIndex,]
test = covs[testIndex,]
covs_r <- cbind(covs, "random" = sample(1:1074, 1074))
covs_r <- as.data.frame(covs_r)
covs_r <- covs_r[order(covs_r$random),]
covs_r$random <- NULL
covs_r
covstest_1 <- covs_r[c(1:107),]
covstest_1
covs<-covs_r
covstest_1 <- covs[c(1:107),]
covstest_2<- covs[c(108:215),]
covstest_3<- covs[c(216:323),]
covstest_4<- covs[c(324:431),]
covstest_5<- covs[c(432:539),]
covstest_6<- covs[c(540:647),]
covstest_7<- covs[c(648:755),]
covstest_8<- covs[c(756:863),]
covstest_9<- covs[c(864:971),]
covstest_10<- covs[c(972:1074),]
covsfull_1 <- covs[-(1:107),]
covsfull_2 <- covs[-c(108:215),]
covsfull_3 <- covs[-c(216:323),]
covsfull_4 <- covs[-c(324:431),]
covsfull_5 <- covs[-c(432:539),]
covsfull_6 <- covs[-c(540:647),]
covsfull_7 <- covs[-c(648:755),]
covsfull_8 <- covs[-c(756:863),]
covsfull_9 <- covs[-c(864:971),]
covsfull_10 <- covs[-c(972:1074),]
set.seed(100)
covs_r <- cbind(covs, "random" = sample(1:1074, 1074))
covs_r <- as.data.frame(covs_r)
covs_r <- covs_r[order(covs_r$random),]
covs_r$random <- NULL
covs<-covs_r
treat_r <- cbind(treat, "random" = sample(1:1074, 1074))
treat_r <- as.data.frame(treat_r)
treat_r <- treat_r[order(treat_r$random),]
treat_r$random <- NULL
treat<-treat_r
treattest_1 <- treat[c(1:107),]
treattest_2<- treat[c(108:215),]
treattest_3<- treat[c(216:323),]
treattest_4<-treat[c(324:431),]
treattest_5<- treat[c(432:539),]
treattest_6<- treat[c(540:647),]
treattest_7<- treat[c(648:755),]
treattest_8<- treat[c(756:863),]
treattest_9<- treat[c(864:971),]
treattest_10<- treat[c(972:1074),]
# Def of 9/10 for treat
treatfull_1 <-treat[-c(1:107),]
treatfull_2 <- treat[-c(108:215),]
treatfull_3 <- treat[-c(216:323),]
treatfull_4 <- treat[-c(324:431),]
treatfull_5 <- treat[-c(432:539),]
treatfull_6 <- treat[-c(540:647),]
treatfull_7 <- treat[-c(648:755),]
treatfull_8 <- treat[-c(756:863),]
treatfull_9 <- treat[-c(864:971),]
treatfull_10 <- treat[-c(972:1074),]
covstest_1
treattest_1
covs<- cbind(dem, rep, lib, cons) #line 373 of rep code
X <- covs #line 432 of repcode
Xfull <- model.matrix(~X*treat)
xfull_letsSee <- cbind(covsfull_1, treatfull_1,"random" = sample(1:1074, 1074) )
covsfull_1 <- covs[-(1:107),]
treatfull_1 <-treat[-c(1:107),]
xfull_letsSee <- cbind(covsfull_1, treatfull_1,"random" = sample(1:1074, 1074) )
Y = ifelse(svdat$approval<3, 1, 0)
data <- cbind(covs, treat, Y)
set.seed(100)
data_r <- cbind(data, "random" = sample(1:1074, 1074))
data_r <- as.data.frame(data_r)
data_r <- data_r[order(data_r$random),]
data_r$random <- NULL
data_r <- data
data_r
covstest_1 <- data[c(1:107),]
covstest_2<- data[c(108:215),]
covstest_3<- data[c(216:323),]
covstest_4<- data[c(324:431),]
covstest_5<- data[c(432:539),]
covstest_6<- data[c(540:647),]
covstest_7<- data[c(648:755),]
covstest_8<- data[c(756:863),]
covstest_9<- data[c(864:971),]
covstest_10<- data[c(972:1074),]
# Def of 9/10 for covs
covsfull_1 <- data[-(1:107),]
covsfull_2 <- data[-c(108:215),]
covsfull_3 <- data[-c(216:323),]
covsfull_4 <- data[-c(324:431),]
covsfull_5 <- data[-c(432:539),]
covsfull_6 <- data[-c(540:647),]
covsfull_7 <- data[-c(648:755),]
covsfull_8 <- data[-c(756:863),]
covsfull_9 <- data[-c(864:971),]
covsfull_10 <- data[-c(972:1074),]
set.seed(100) #can provide any number for seed
nall = nrow(data) #total number of rows in data
ntrain = floor(0.9 * nall) # number of rows for train,70%
ntest = floor(0.1* nall) # number of rows for test, 30%
index = seq(1:nall)
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
train = data[trainIndex,]
test = data[test,]
nrow(data)
0.9 * nall
set.seed(100) #can provide any number for seed
nall = nrow(data) #total number of rows in data
ntrain = floor(0.9 * nall) # number of rows for train,70%
ntest = floor(0.1* nall) # number of rows for test, 30%
index = seq(1:nall)
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
train = data[trainIndex,]
test = data[test,]
set.seed(100) #can provide any number for seed
nall = nrow(data) #total number of rows in data
ntrain = floor(0.9 * nall) # number of rows for train,70%
ntest = floor(0.1* nall) # number of rows for test, 30%
index = seq(1:nall)
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
train = data[trainIndex,]
test = data[testIndex,]
seq(1:nall)
index = seq(1:ntrain)
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
train = data[trainIndex,]
test = data[testIndex,]
set.seed(100) #can provide any number for seed
nall = nrow(data) #total number of rows in data
ntrain = floor(0.9 * nall) # number of rows for train,70%
ntest = floor(0.1* nall) # number of rows for test, 30%
index = seq(1:ntrain)
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
train = data[trainIndex,]
test = data[testIndex,]
testIndex = index[-train]
index
library("caret")
trainIndex = createDataPartition(data,
p=0.9, list=FALSE,times=1)
trainIndex = createDataPartition(data,
p=0.9, list=FALSE,times=1)
data_r
trainIndex = createDataPartition(data,
p=0.9, list=FALSE,times=1)
trainIndex = createDataPartition(data$test,
p=0.9, list=FALSE,times=1)
set.seed(100) #can provide any number for seed
nall = nrow(data) #total number of rows in data
ntrain = floor(0.9 * nall) # number of rows for train,70%
ntest = floor(0.1* nall) # number of rows for test, 30%
index = seq(1:ntrain)
trainIndex = sample(index, ntrain) #train data set
testIndex = index[-train]
train = data[trainIndex,]
test = data[testIndex,]
sample(data, .9 *nrow(data), replace = FALSE)
sample(data, (.9 *nrow(data)), replace = FALSE)
nrow(data)
.9 *nrow(data)
sample(data, (.9 *nrow(data)))
covs_sample <- sample.int(n = nrow(covs_r), size = floor(.9*nrow(covs_r)), replace = F)
covs_sample
