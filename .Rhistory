<<<<<<< HEAD
mat1 <- matrix(abs(rnorm(2500))+pi, ncol=50)
head(mat1)[,1:5]
apply(mat1, 1, function(x) sum(x))
rowSums(mat1)
microbenchmark(apply(mat1, 1, function(x) sum(x)),
rowSums(mat1))
apply(mat1, 2, function(x) mean(x))
colMeans(mat1)
microbenchmark(apply(mat1, 2, function(x) mean(x)),
colMeans(mat1))
mat2 <- matrix(sample(1:7, 90000, replace=T), ncol=300)
mat3 <- matrix(sample(2:6, 90000, replace=T), ncol=300)
ys <- sample(3:5, 300, replace=T)
all.equal(mat2 %*% mat3 %*% ys , mat2 %*% (mat3 %*% ys))
microbenchmark(mat2 %*% mat3 %*% ys,
mat2 %*% (mat3 %*% ys))
mat4 <- matrix(1:4, ncol=2)
mat5 <- matrix(5:8, ncol=2)
microbenchmark(t(mat4)%*%mat5,
crossprod(mat4, mat5))
random_states <- function() {
paste(sample(state.name,10,replace =TRUE),collapse ="")
}
states10 <- replicate(10, random_states())
states10
states100 <- replicate(100, random_states())
collapse <- function(states) {
out <- ""
for (x in states) {
out <- paste0(out, x) # same as paste(..., sep="", collapse)
}
out
}
microbenchmark(
"loop10" = collapse(states10),
"vec10" = paste(states10, collapse =""),
"loop100" = collapse(states100),
"vec100" = paste(states100, collapse ="")
)
library(parallel)
cores <- detectCores()
cores
pause <- function(i) {
function(x) Sys.sleep(i)
}
cluster <- makePSOCKcluster(cores)
microbenchmark(
parLapply(cluster, 1:4, pause(0.25)),
lapply(1:4, pause(0.25)),
times=10
)
library(plyr)
bigmat <- matrix(rnorm(90000), ncol=300)
dim(bigmat)
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
"vectorized" = colSums(bigmat),
times=20
)
library(foreach)
library(foreach)
library(doSNOW)
registerDoSNOW(makeCluster(2, type = "SOCK")) # set to two cores
getDoParWorkers() # check number of cores
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
times=20
)
library(readr)
Multiple_Cause_of_Death_1999_2016 <- read_csv("~/Opioid Public Policy Project/Multiple Cause of Death, 1999-2016.txt")
View(Multiple_Cause_of_Death_1999_2016)
View(Multiple_Cause_of_Death_1999_2016)
=======
lines(high.quality.x, high.quality.y, lty=2, col="red")
lines(low.quality.x, low.quality.y, lty=3, col="blue")
lines(density(voteshare*100, na.rm=TRUE), lty=1) # lines will add a line to an exiting plot.  lty=2 specifies a dashed line
?rgb
# this isn't very helpful here, but let's look
col1 <- rgb(1, 0, 0, alpha=.05)
col2 <- rgb(0, 0, 1, alpha=.05)
rug(voteshare[chalquality==1]*100, col=col1)
rug(voteshare[chalquality==0]*100, col=col2)
args(plot.default) # see the basics of the plot funciton
# we'll make some pretend data for views of the economy for democrats
#, independents, and Republicans
xaxis<-c(1:12) # think of this as time periods
econ.inds<-c(2,3,3.5, 2, 3, 2.5, 3, 2.5, 3, 3.5,4,4)
econ.reps <- econ.inds + 2
econ.dems <- econ.inds - 1
plot(xaxis, econ.inds) # this is your standard default plot.  The default is type="p"
plot(xaxis, econ.inds, type="l") # lines
plot(xaxis, econ.inds, type="n") #nothing
plot(xaxis, econ.inds, type="o") # both points and lines
plot(xaxis, econ.inds, type="h") # a sort of fake histogram with lines
plot(xaxis, econ.inds, type="s") # stair steps
# another way to do this would be
plot(NULL, xlim=c(1, 12), ylim=c(2,4))
points(xaxis, econ.inds, type="b")
#or
plot(NULL, xlim=c(1, 12), ylim=c(2,4))
points(xaxis, econ.inds, type="l")
#or
plot(NULL, xlim=c(1, 12), ylim=c(2,4))
lines(xaxis, econ.inds)
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
RMSEforModel = function(x,y, test.indexes = sample(length(y),as.integer(length(y)/10))){
# NOTE: THIS IS NOT DOING CROSS-VALIDATION right now (for most models)
#test.indexes = sample(1074,107)
x.train = x[-test.indexes,]
x.test = x[test.indexes,]
y.train = y[-test.indexes]
y.test = y[test.indexes]
library(glmnet)
##Now predict them
logist<- function(x){
ff<- 1/(1 + exp(-x))
return(ff)
}
# Lasso
fit1<- cv.glmnet(x = x.train, y = y.train, alpha=1, family='binomial', type='mse')
best.lambda = fit1$lambda.min
fit1.predict = predict(fit1, s= best.lambda, newx = x.test)
#fit1.RMSE = sqrt(mean((fit1.predict-y.test)^2))
fit1.logistPred = logist(fit1.predict)
fit1.logistPred.RMSE = sqrt(mean((fit1.logistPred-y.test)^2))
# Elastic Net, Alpha = .5
fit2<- cv.glmnet(y = y.train, x= x.train, alpha=0.5, family='binomial', type='mse')
best.lambda = fit2$lambda.min
fit2.predict = predict(fit2, s= best.lambda, newx = x.test)
#fit2.RMSE = sqrt(mean((fit2.predict-y.test)^2))
fit2.logistPred = logist(fit2.predict)
fit2.logistPred.RMSE = sqrt(mean((fit2.logistPred-y.test)^2))
# Elastic Net, Alpha = .25
fit3<- cv.glmnet(y = y.train, x= x.train, alpha=0.25, family='binomial', type='mse')
best.lambda = fit3$lambda.min
fit3.predict = predict(fit3, s= best.lambda, newx = x.test)
#fit3.RMSE = sqrt(mean((fit3.predict-y.test)^2))
fit3.logistPred = logist(fit3.predict)
fit3.logistPred.RMSE = sqrt(mean((fit3.logistPred-y.test)^2))
# Fit4 not published in paper -- so not doing it
#fit4<- cv.glmnet(y = Y, x= Xfull, alpha=0, family='binomial', type='mse')
## Skipping FindIt since documentation changed (per)
# Bayesian GLM -- Revisit -- probably not working right
#Not sure if I should directly predict via linear estiamtion, or use a logistic regression
# Prediction Function Doesn't really work for this
#install.packages("arm")
library(arm)
fit6<- bayesglm(y.train~x.train-1, family=binomial(link=logit))
#fit6.predict = x.test%*%fit6$coefficients
fit6.predict2 = logist(x.test%*%fit6$coefficients)
#fit6.1 = bayesglm(y~x-1, family=binomial(link=logit))
#fit6.RMSE = sqrt(mean(resid(fit6.1)^2))
fit6.logistPred.RMSE = sqrt(mean((fit6.predict2-y.test)^2))
# Fit 7 = Boosted Trees is not published ### SKipping
# Fit 8 = BART
library(BayesTree)
fit8<- bart(x.train=x.train, y.train=factor(y.train), x.test=x.test, ndpost=1000, nskip=500, usequants=T)
fit8.pred<- pnorm(apply(fit8$yhat.test, 2, mean))
fit8.rmse = sqrt(mean((fit8.pred-y.test)^2))
# Fit 9  = RandomForest
library(randomForest)
fit9<- randomForest(y = factor(y.train), x = x.train)
X.test.forest = x.test
`colnames<-`(X.test.forest,colnames(x.train))
#Only works for X.test??
fit9.pred.raw = predict(fit9,newdata = X.test.forest,type = "prob" )
fit9.pred = fit9.pred.raw[,2]
fit9.rmse = sqrt(mean((fit9.pred-y.test)^2))
# Fit 10 = Skipped in Paper and SLF_round2 code
# Fit 11 = KRLS
library(KRLS)
fit11<- krls(X = x.train[,-1], y = y.train, derivative=F)
fit11.predict = predict(fit11,newdata = x.test[,-1])$fit
fit11.rmse = sqrt(mean((fit11.predict-y.test)^2))
# Fit 12 = SVM-SMO
# library(rJava)
#  .jinit(parameters="-Xmx4g")
# library(RWeka)
#  subset.index = (1:length(y))[-test.indexes]
# fit12 <- SMO(y ~ ., data = data.frame(y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
#fit12 <- SMO(Y ~ ., data = data.frame(Y=factor(Y),Xfull), control = Weka_control(M = TRUE ) , subset = ((1:1074)[-test.indexes]))
# fit12.predict =predict(fit12, newdata= data.frame(x[test.indexes,]), type="probability" )[,2]
#  fit12.RMSE = sqrt(mean((fit12.predict-y.test)^2))
# Fit 13 = Simple Mean
fit13.predict = mean(y.train)
fit13.RSME= sqrt(mean((fit13.predict-y.test)^2))
#"SVM-SMO",
<<<<<<< HEAD
models = cbind("Lasso", "Elastic Net (a = .5)","Elastic Net (a = .25)", "Bayesian GLM", "BART", "Random Forest", "KRLS",  "Simple Average")
#fit12.RMSE,
RMSE.all = cbind(fit1.logistPred.RMSE,fit2.logistPred.RMSE,fit3.logistPred.RMSE,fit6.logistPred.RMSE,fit8.rmse,fit9.rmse,fit11.rmse, fit13.RSME)
#models,
return(data.frame(RMSE.all))
}
setwd("~/GitHub/Project")
=======
models = rbind("Lasso", "Elastic Net (a = .5)","Elastic Net (a = .25)", "Bayesian GLM", "BART", "Random Forest", "KRLS",  "Simple Average")
#fit12.RMSE,
RMSE.all = rbind(fit1.logistPred.RMSE,fit2.logistPred.RMSE,fit3.logistPred.RMSE,fit6.logistPred.RMSE,fit8.rmse,fit9.rmse,fit11.rmse, fit13.RSME)
#models,
return(data.frame(RMSE.all))
}
#setwd("C:/Users/jgros/documents/GitHub/Project/")
#load("Het_Experiment.Rdata")
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
dem<- ifelse(svdat$pid3l=='Dem', 1, 0)  #line 366-369 of rep code
dem[which(is.na(dem))]<- 0
rep<- ifelse(svdat$pid3l=='Rep', 1, 0)
rep[which(is.na(rep))]<- 0
cons<- ifelse(svdat$ideo3<3, 1, 0) #line 230-231 of rep code
lib<- ifelse(svdat$ideo3==4|svdat$ideo3==5, 1, 0)
lib[which(is.na(lib))]<- 0 #line 370-371 of rep code
cons[which(is.na(cons))]<- 0
############ Defining treats
type.mat<- matrix(0, nrow = 1074, ncol=7)
colnames(type.mat)<- sort(unique(as.character(svdat$cond.type)))
for(z in 1:nrow(type.mat)){
type.mat[z,which(colnames(type.mat)==svdat$cond.type[z])]<- 1
<<<<<<< HEAD
=======
}
type.mat.final<- type.mat[,-1]
types<- sort(unique(as.character(svdat$cond.type)))
type.num<- match(svdat$cond.type, types)
number<- c('control', '$20 million', '$50 thousand')
amount.num<- match(svdat$cond.money, number)
request<- c('control', 'requested', 'secured', 'will request')
stage.num<- match(svdat$cond.stage, request)
party<- c('control', 'a Republican', 'a Democrat')
party.num<- match(svdat$cond.party, party)
along<- c('control', 'alone', 'w/ Rep', 'w/ Dem')
along.num<- match(svdat$cond.alongWith, along)
num.mat<- matrix(0, nrow=1074, ncol=3)
colnames(num.mat)<- number
for(z in 1:nrow(num.mat)){
num.mat[z,which(colnames(num.mat)==svdat$cond.money[z])]<- 1
}
num.mat.final<- num.mat[,-1]
stage.mat<- matrix(0, nrow=1074, ncol=4)
colnames(stage.mat)<- request
for(z in 1:nrow(stage.mat)){
stage.mat[z,which(colnames(stage.mat)==svdat$cond.stage[z])]<- 1
}
stage.mat.final<- stage.mat[,-1]
party.mat<- matrix(0, nrow=1074, ncol=3)
colnames(party.mat)<- party
for(z in 1:nrow(party.mat)){
party.mat[z, which(colnames(party.mat)==svdat$cond.party[z])]<- 1
}
party.mat.final<- party.mat[,-1]
along.mat<- matrix(0, nrow=1074, ncol=4)
colnames(along.mat)<- 	along
for(z in 1:nrow(along.mat)){
along.mat[z,which(colnames(along.mat)==svdat$cond.alongWith[z])]<- 1
}
along.mat.final<- along.mat[,-1]
treats<- cbind(type.mat.final, num.mat.final[,1], stage.mat.final[,1:2],party.mat.final[,1],
along.mat.final[,1:2], type.mat.final[,1:5]*num.mat.final[,1], type.mat.final[,1:5]*stage.mat.final[,1],
type.mat.final[,1:5]*stage.mat.final[,2], type.mat.final[,1:5]*party.mat.final[,1], type.mat.final[,1:5]*along.mat.final[,1],
type.mat.final[,1:5]*along.mat.final[,2], num.mat.final[,1]*stage.mat.final[,1], num.mat.final[,1]*stage.mat.final[,2],
num.mat.final[,1]*party.mat.final[,1], num.mat.final[,1]*along.mat.final[,1], num.mat.final[,1]*along.mat.final[,2],
stage.mat.final[,1:2]*party.mat.final[,1], stage.mat.final[,1:2]*along.mat.final[,1],
stage.mat.final[,1:2]*along.mat.final[,2], party.mat.final[,1]*along.mat.final[,1], party.mat.final[,1]*along.mat.final[,2] )
treat<- treats #line 448 of rep code
### Defining the X variable
covs<- cbind(dem, rep, lib, cons) #line 373 of rep code
X <- covs #line 432 of repcode
Xfull <- model.matrix(~X*treat)
## line 391 of rep code
#Defining the Y variable
#line 432 of rep code
Y<- approve_bi<- ifelse(svdat$approval<3, 1, 0) #line 292 of rep code
# One Query
df<-RMSEforModel(Xfull,Y)
# With Cross Validation
indexes = 1:1074
indexes = sample(indexes)
indexes.matrix = matrix(indexes,nrow=10)
#Makes data frame with all 10 RMSE
for(i in 1:10){
df$i <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
}
setwd("C:/Users/jgros/documents/GitHub/Project/")
load("Het_Experiment.Rdata")
dem<- ifelse(svdat$pid3l=='Dem', 1, 0)  #line 366-369 of rep code
dem[which(is.na(dem))]<- 0
rep<- ifelse(svdat$pid3l=='Rep', 1, 0)
rep[which(is.na(rep))]<- 0
cons<- ifelse(svdat$ideo3<3, 1, 0) #line 230-231 of rep code
lib<- ifelse(svdat$ideo3==4|svdat$ideo3==5, 1, 0)
lib[which(is.na(lib))]<- 0 #line 370-371 of rep code
cons[which(is.na(cons))]<- 0
type.mat<- matrix(0, nrow = 1074, ncol=7)
colnames(type.mat)<- sort(unique(as.character(svdat$cond.type)))
for(z in 1:nrow(type.mat)){
type.mat[z,which(colnames(type.mat)==svdat$cond.type[z])]<- 1
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
}
type.mat.final<- type.mat[,-1]
types<- sort(unique(as.character(svdat$cond.type)))
type.num<- match(svdat$cond.type, types)
number<- c('control', '$20 million', '$50 thousand')
amount.num<- match(svdat$cond.money, number)
request<- c('control', 'requested', 'secured', 'will request')
stage.num<- match(svdat$cond.stage, request)
party<- c('control', 'a Republican', 'a Democrat')
party.num<- match(svdat$cond.party, party)
along<- c('control', 'alone', 'w/ Rep', 'w/ Dem')
along.num<- match(svdat$cond.alongWith, along)
num.mat<- matrix(0, nrow=1074, ncol=3)
colnames(num.mat)<- number
for(z in 1:nrow(num.mat)){
num.mat[z,which(colnames(num.mat)==svdat$cond.money[z])]<- 1
}
num.mat.final<- num.mat[,-1]
stage.mat<- matrix(0, nrow=1074, ncol=4)
colnames(stage.mat)<- request
for(z in 1:nrow(stage.mat)){
stage.mat[z,which(colnames(stage.mat)==svdat$cond.stage[z])]<- 1
}
stage.mat.final<- stage.mat[,-1]
party.mat<- matrix(0, nrow=1074, ncol=3)
colnames(party.mat)<- party
for(z in 1:nrow(party.mat)){
party.mat[z, which(colnames(party.mat)==svdat$cond.party[z])]<- 1
}
party.mat.final<- party.mat[,-1]
along.mat<- matrix(0, nrow=1074, ncol=4)
colnames(along.mat)<- 	along
for(z in 1:nrow(along.mat)){
along.mat[z,which(colnames(along.mat)==svdat$cond.alongWith[z])]<- 1
}
along.mat.final<- along.mat[,-1]
treats<- cbind(type.mat.final, num.mat.final[,1], stage.mat.final[,1:2],party.mat.final[,1],
along.mat.final[,1:2], type.mat.final[,1:5]*num.mat.final[,1], type.mat.final[,1:5]*stage.mat.final[,1],
type.mat.final[,1:5]*stage.mat.final[,2], type.mat.final[,1:5]*party.mat.final[,1], type.mat.final[,1:5]*along.mat.final[,1],
type.mat.final[,1:5]*along.mat.final[,2], num.mat.final[,1]*stage.mat.final[,1], num.mat.final[,1]*stage.mat.final[,2],
num.mat.final[,1]*party.mat.final[,1], num.mat.final[,1]*along.mat.final[,1], num.mat.final[,1]*along.mat.final[,2],
stage.mat.final[,1:2]*party.mat.final[,1], stage.mat.final[,1:2]*along.mat.final[,1],
stage.mat.final[,1:2]*along.mat.final[,2], party.mat.final[,1]*along.mat.final[,1], party.mat.final[,1]*along.mat.final[,2] )
treat<- treats #line 448 of rep code
### Defining the X variable
covs<- cbind(dem, rep, lib, cons) #line 373 of rep code
X <- covs #line 432 of repcode
Xfull <- model.matrix(~X*treat)
<<<<<<< HEAD
## line 391 of rep code
#Defining the Y variable
#line 432 of rep code
Y<- approve_bi<- ifelse(svdat$approval<3, 1, 0) #line 292 of rep code
load("~/GitHub/Project/Het_Experiment.RData")
=======
#line 432 of rep code
Y<- approve_bi<- ifelse(svdat$approval<3, 1, 0) #line 292 of rep code
# One Query
df<-RMSEforModel(Xfull,Y)
df
for(i in 1:2){
df$i <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
}
df
source('~/GitHub/Project/RMSEforModel.R', echo=TRUE)
for(i in 1:10){
df$i <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
}
x = Xfull
y = Y
test.indexes
test.indexes = indexes.matrix[1,]
test.indexes
library(rJava)
.jinit(parameters="-Xmx4g")
library(RWeka)
subset.index = (1:length(y))[-test.indexes]
fit12 <- SMO(y ~ ., data = data.frame(y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
fit12.predict =predict(fit12, newdata= data.frame(x[test.indexes,]), type="probability" )[,2]
fit12.RMSE = sqrt(mean((fit12.predict-y.test)^2))
fit12 <- SMO(Y ~ ., data = data.frame(Y=factor(Y),Xfull), control = Weka_control(M = TRUE ) , subset = ((1:1074)[-test.indexes]))
fit12 <- SMO(y ~ ., data = data.frame(y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
fit12.1 <- SMO(Y ~ ., data = data.frame(Y=factor(Y),Xfull), control = Weka_control(M = TRUE ) , subset = ((1:1074)[-test.indexes]))
fit12.1==fit12
Preds.All = rbind(fit1.logistPred,fit2.logistPred,fit3.logistPred,fit6.predict,fit8.pred,fit9.pred,fit11.predict,fit13.predict)
for(i in 1:1){
df$i <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
}
# df$i <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
preds <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
preds
preds
Preds.All
Preds.All
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
RMSEforModel = function(x,y, test.indexes = sample(length(y),as.integer(length(y)/10))){
# NOTE: THIS IS NOT DOING CROSS-VALIDATION right now (for most models)
#test.indexes = sample(1074,107)
x.train = x[-test.indexes,]
x.test = x[test.indexes,]
y.train = y[-test.indexes]
y.test = y[test.indexes]
library(glmnet)
##Now predict them
logist<- function(x){
ff<- 1/(1 + exp(-x))
return(ff)
}
##### Take out this Cross-Validation -- just do CV onces for everything to make run faster
# Lasso
fit1<- cv.glmnet(x = x.train, y = y.train, alpha=1, family='binomial', type='mse')
best.lambda = fit1$lambda.min
fit1.predict = predict(fit1, s= best.lambda, newx = x.test)
fit1.logistPred = logist(fit1.predict)
fit1.logistPred.RMSE = sqrt(mean((fit1.logistPred-y.test)^2))
# Elastic Net, Alpha = .5
fit2<- cv.glmnet(y = y.train, x= x.train, alpha=0.5, family='binomial', type='mse')
best.lambda = fit2$lambda.min
fit2.predict = predict(fit2, s= best.lambda, newx = x.test)
fit2.logistPred = logist(fit2.predict)
fit2.logistPred.RMSE = sqrt(mean((fit2.logistPred-y.test)^2))
# Elastic Net, Alpha = .25
fit3<- cv.glmnet(y = y.train, x= x.train, alpha=0.25, family='binomial', type='mse')
best.lambda = fit3$lambda.min
fit3.predict = predict(fit3, s= best.lambda, newx = x.test)
fit3.logistPred = logist(fit3.predict)
fit3.logistPred.RMSE = sqrt(mean((fit3.logistPred-y.test)^2))
# Fit4 not published in paper -- so not doing it
#fit4<- cv.glmnet(y = Y, x= Xfull, alpha=0, family='binomial', type='mse')
## Skipping FindIt since documentation changed (per)
# Bayesian GLM -- Revisit -- probably not working right
library(arm)
fit6<- bayesglm(y.train~x.train-1, family=binomial(link=logit))
fit6.predict = logist(x.test%*%fit6$coefficients)
fit6.logistPred.RMSE = sqrt(mean((fit6.predict-y.test)^2))
# Fit 7 = Boosted Trees is not published ### SKipping
# Fit 8 = BART
library(BayesTree)
fit8<- bart(x.train=x.train, y.train=factor(y.train), x.test=x.test, ndpost=1000, nskip=500, usequants=T)
fit8.pred<- pnorm(apply(fit8$yhat.test, 2, mean))
fit8.rmse = sqrt(mean((fit8.pred-y.test)^2))
# Fit 9  = RandomForest
library(randomForest)
fit9<- randomForest(y = factor(y.train), x = x.train)
X.test.forest = x.test
`colnames<-`(X.test.forest,colnames(x.train))
fit9.pred.raw = predict(fit9,newdata = X.test.forest,type = "prob" )
fit9.pred = fit9.pred.raw[,2]
fit9.rmse = sqrt(mean((fit9.pred-y.test)^2))
# Fit 10 = Skipped in Paper and SLF_round2 code
# Fit 11 = KRLS
library(KRLS)
fit11<- krls(X = x.train[,-1], y = y.train, derivative=F)
fit11.predict = predict(fit11,newdata = x.test[,-1])$fit
fit11.rmse = sqrt(mean((fit11.predict-y.test)^2))
# Fit 12 = SVM-SMO
<<<<<<< HEAD
# library(rJava)
#  .jinit(parameters="-Xmx4g")
# library(RWeka)
#  subset.index = (1:length(y))[-test.indexes]
# fit12 <- SMO(y ~ ., data = data.frame(y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
#fit12 <- SMO(Y ~ ., data = data.frame(Y=factor(Y),Xfull), control = Weka_control(M = TRUE ) , subset = ((1:1074)[-test.indexes]))
# fit12.predict =predict(fit12, newdata= data.frame(x[test.indexes,]), type="probability" )[,2]
#  fit12.RMSE = sqrt(mean((fit12.predict-y.test)^2))
# Fit 13 = Simple Mean
fit13.predict = mean(y.train)
fit13.RSME= sqrt(mean((fit13.predict-y.test)^2))
#"SVM-SMO",
models = cbind("Lasso", "Elastic Net (a = .5)","Elastic Net (a = .25)", "Bayesian GLM", "BART", "Random Forest", "KRLS",  "Simple Average")
#fit12.RMSE,
RMSE.all = cbind(fit1.logistPred.RMSE,fit2.logistPred.RMSE,fit3.logistPred.RMSE,fit6.logistPred.RMSE,fit8.rmse,fit9.rmse,fit11.rmse, fit13.RSME)
#models,
return(data.frame(RMSE.all))
}
#setwd("C:/Users/jgros/documents/GitHub/Project/")
#load("Het_Experiment.Rdata")
dem<- ifelse(svdat$pid3l=='Dem', 1, 0)  #line 366-369 of rep code
dem[which(is.na(dem))]<- 0
rep<- ifelse(svdat$pid3l=='Rep', 1, 0)
rep[which(is.na(rep))]<- 0
cons<- ifelse(svdat$ideo3<3, 1, 0) #line 230-231 of rep code
lib<- ifelse(svdat$ideo3==4|svdat$ideo3==5, 1, 0)
lib[which(is.na(lib))]<- 0 #line 370-371 of rep code
cons[which(is.na(cons))]<- 0
############ Defining treats
type.mat<- matrix(0, nrow = 1074, ncol=7)
colnames(type.mat)<- sort(unique(as.character(svdat$cond.type)))
for(z in 1:nrow(type.mat)){
type.mat[z,which(colnames(type.mat)==svdat$cond.type[z])]<- 1
}
type.mat.final<- type.mat[,-1]
types<- sort(unique(as.character(svdat$cond.type)))
type.num<- match(svdat$cond.type, types)
number<- c('control', '$20 million', '$50 thousand')
amount.num<- match(svdat$cond.money, number)
request<- c('control', 'requested', 'secured', 'will request')
stage.num<- match(svdat$cond.stage, request)
party<- c('control', 'a Republican', 'a Democrat')
party.num<- match(svdat$cond.party, party)
along<- c('control', 'alone', 'w/ Rep', 'w/ Dem')
along.num<- match(svdat$cond.alongWith, along)
num.mat<- matrix(0, nrow=1074, ncol=3)
colnames(num.mat)<- number
for(z in 1:nrow(num.mat)){
num.mat[z,which(colnames(num.mat)==svdat$cond.money[z])]<- 1
}
num.mat.final<- num.mat[,-1]
stage.mat<- matrix(0, nrow=1074, ncol=4)
colnames(stage.mat)<- request
for(z in 1:nrow(stage.mat)){
stage.mat[z,which(colnames(stage.mat)==svdat$cond.stage[z])]<- 1
}
stage.mat.final<- stage.mat[,-1]
party.mat<- matrix(0, nrow=1074, ncol=3)
colnames(party.mat)<- party
for(z in 1:nrow(party.mat)){
party.mat[z, which(colnames(party.mat)==svdat$cond.party[z])]<- 1
}
party.mat.final<- party.mat[,-1]
along.mat<- matrix(0, nrow=1074, ncol=4)
colnames(along.mat)<- 	along
for(z in 1:nrow(along.mat)){
along.mat[z,which(colnames(along.mat)==svdat$cond.alongWith[z])]<- 1
}
along.mat.final<- along.mat[,-1]
treats<- cbind(type.mat.final, num.mat.final[,1], stage.mat.final[,1:2],party.mat.final[,1],
along.mat.final[,1:2], type.mat.final[,1:5]*num.mat.final[,1], type.mat.final[,1:5]*stage.mat.final[,1],
type.mat.final[,1:5]*stage.mat.final[,2], type.mat.final[,1:5]*party.mat.final[,1], type.mat.final[,1:5]*along.mat.final[,1],
type.mat.final[,1:5]*along.mat.final[,2], num.mat.final[,1]*stage.mat.final[,1], num.mat.final[,1]*stage.mat.final[,2],
num.mat.final[,1]*party.mat.final[,1], num.mat.final[,1]*along.mat.final[,1], num.mat.final[,1]*along.mat.final[,2],
stage.mat.final[,1:2]*party.mat.final[,1], stage.mat.final[,1:2]*along.mat.final[,1],
stage.mat.final[,1:2]*along.mat.final[,2], party.mat.final[,1]*along.mat.final[,1], party.mat.final[,1]*along.mat.final[,2] )
treat<- treats #line 448 of rep code
### Defining the X variable
covs<- cbind(dem, rep, lib, cons) #line 373 of rep code
X <- covs #line 432 of repcode
Xfull <- model.matrix(~X*treat)
## line 391 of rep code
#Defining the Y variable
#line 432 of rep code
Y<- approve_bi<- ifelse(svdat$approval<3, 1, 0) #line 292 of rep code
df<-RMSEforModel(Xfull,Y)
df
View(df)
indexes = 1:1074
indexes = sample(indexes)
indexes.matrix = matrix(indexes,nrow=10)
for(i in 1:10){
df$i <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
}
View(df)
indexes.matrix
View(indexes.matrix)
indexes.matrix[4,]
RMSEforModel = function(x,y, test.indexes = sample(length(y),as.integer(length(y)/10))){
# NOTE: THIS IS NOT DOING CROSS-VALIDATION right now (for most models)
#test.indexes = sample(1074,107)
x.train = x[-test.indexes,]
x.test = x[test.indexes,]
y.train = y[-test.indexes]
y.test = y[test.indexes]
library(glmnet)
##Now predict them
logist<- function(x){
ff<- 1/(1 + exp(-x))
return(ff)
}
=======
library(rJava)
.jinit(parameters="-Xmx4g")
library(RWeka)
subset.index = (1:length(y))[-test.indexes]
fit12 <- SMO(y ~ ., data = data.frame(y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
fit12.predict =predict(fit12, newdata= data.frame(x[test.indexes,]), type="probability" )[,2]
fit12.RMSE = sqrt(mean((fit12.predict-y.test)^2))
# Fit 13 = Simple Mean
fit13.predict = mean(y.train)
fit13.RSME= sqrt(mean((fit13.predict-y.test)^2))
data.frame
models = rbind("Lasso", "Elastic Net (a = .5)","Elastic Net (a = .25)", "Bayesian GLM", "BART", "Random Forest", "KRLS", "SVM_SMO", "Simple Average")
Preds.All = rbind(fit1.logistPred,fit2.logistPred,fit3.logistPred,fit6.predict,fit8.pred,fit9.pred,fit11.predict,fit13.predict)
RMSE.all = rbind(fit1.logistPred.RMSE,fit2.logistPred.RMSE,fit3.logistPred.RMSE,fit6.logistPred.RMSE,fit8.rmse,fit9.rmse,fit11.rmse, fit13.RSME)
#models,
#return(data.frame(RMSE.all))
return(Preds.All)
}
for(i in 1:1){
# df$i <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
preds <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
}
preds
preds
Preds.All
Preds.All
Preds.All = rbind(fit1.logistPred,fit2.logistPred,fit3.logistPred,fit6.predict,fit8.pred,fit9.pred,fit11.predict,fit13.predict)
#test.indexes = sample(1074,107)
x.train = x[-test.indexes,]
x.test = x[test.indexes,]
y.train = y[-test.indexes]
y.test = y[test.indexes]
library(glmnet)
##Now predict them
logist<- function(x){
ff<- 1/(1 + exp(-x))
return(ff)
}
##### Take out this Cross-Validation -- just do CV onces for everything to make run faster
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
# Lasso
fit1<- cv.glmnet(x = x.train, y = y.train, alpha=1, family='binomial', type='mse')
best.lambda = fit1$lambda.min
fit1.predict = predict(fit1, s= best.lambda, newx = x.test)
<<<<<<< HEAD
#fit1.RMSE = sqrt(mean((fit1.predict-y.test)^2))
fit1.logistPred = logist(fit1.predict)
fit1.logistPred.RMSE = sqrt(mean((fit1.logistPred-y.test)^2))
=======
fit1.logistPred = logist(fit1.predict)
fit1.logistPred.RMSE = sqrt(mean((fit1.logistPred-y.test)^2))
fit2.predict = predict(fit2, s= best.lambda, newx = x.test)
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
# Elastic Net, Alpha = .5
fit2<- cv.glmnet(y = y.train, x= x.train, alpha=0.5, family='binomial', type='mse')
best.lambda = fit2$lambda.min
fit2.predict = predict(fit2, s= best.lambda, newx = x.test)
<<<<<<< HEAD
#fit2.RMSE = sqrt(mean((fit2.predict-y.test)^2))
=======
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
fit2.logistPred = logist(fit2.predict)
fit2.logistPred.RMSE = sqrt(mean((fit2.logistPred-y.test)^2))
# Elastic Net, Alpha = .25
fit3<- cv.glmnet(y = y.train, x= x.train, alpha=0.25, family='binomial', type='mse')
best.lambda = fit3$lambda.min
fit3.predict = predict(fit3, s= best.lambda, newx = x.test)
<<<<<<< HEAD
#fit3.RMSE = sqrt(mean((fit3.predict-y.test)^2))
fit3.logistPred = logist(fit3.predict)
fit3.logistPred.RMSE = sqrt(mean((fit3.logistPred-y.test)^2))
# Fit4 not published in paper -- so not doing it
#fit4<- cv.glmnet(y = Y, x= Xfull, alpha=0, family='binomial', type='mse')
## Skipping FindIt since documentation changed (per)
# Bayesian GLM -- Revisit -- probably not working right
#Not sure if I should directly predict via linear estiamtion, or use a logistic regression
# Prediction Function Doesn't really work for this
#install.packages("arm")
library(arm)
fit6<- bayesglm(y.train~x.train-1, family=binomial(link=logit))
#fit6.predict = x.test%*%fit6$coefficients
fit6.predict2 = logist(x.test%*%fit6$coefficients)
#fit6.1 = bayesglm(y~x-1, family=binomial(link=logit))
#fit6.RMSE = sqrt(mean(resid(fit6.1)^2))
fit6.logistPred.RMSE = sqrt(mean((fit6.predict2-y.test)^2))
# Fit 7 = Boosted Trees is not published ### SKipping
=======
fit3.logistPred = logist(fit3.predict)
fit3.logistPred.RMSE = sqrt(mean((fit3.logistPred-y.test)^2))
# Bayesian GLM -- Revisit -- probably not working right
library(arm)
fit6<- bayesglm(y.train~x.train-1, family=binomial(link=logit))
fit6.predict = logist(x.test%*%fit6$coefficients)
fit6.logistPred.RMSE = sqrt(mean((fit6.predict-y.test)^2))
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
# Fit 8 = BART
library(BayesTree)
fit8<- bart(x.train=x.train, y.train=factor(y.train), x.test=x.test, ndpost=1000, nskip=500, usequants=T)
fit8.pred<- pnorm(apply(fit8$yhat.test, 2, mean))
fit8.rmse = sqrt(mean((fit8.pred-y.test)^2))
# Fit 9  = RandomForest
library(randomForest)
fit9<- randomForest(y = factor(y.train), x = x.train)
X.test.forest = x.test
`colnames<-`(X.test.forest,colnames(x.train))
<<<<<<< HEAD
#Only works for X.test??
fit9.pred.raw = predict(fit9,newdata = X.test.forest,type = "prob" )
fit9.pred = fit9.pred.raw[,2]
fit9.rmse = sqrt(mean((fit9.pred-y.test)^2))
# Fit 10 = Skipped in Paper and SLF_round2 code
=======
fit9.pred.raw = predict(fit9,newdata = X.test.forest,type = "prob" )
fit9.pred = fit9.pred.raw[,2]
fit9.rmse = sqrt(mean((fit9.pred-y.test)^2))
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
# Fit 11 = KRLS
library(KRLS)
fit11<- krls(X = x.train[,-1], y = y.train, derivative=F)
fit11.predict = predict(fit11,newdata = x.test[,-1])$fit
<<<<<<< HEAD
fit11.rmse = sqrt(mean((fit11.predict-y.test)^2))
# Fit 12 = SVM-SMO
# library(rJava)
#  .jinit(parameters="-Xmx4g")
# library(RWeka)
#  subset.index = (1:length(y))[-test.indexes]
# fit12 <- SMO(y ~ ., data = data.frame(y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
#fit12 <- SMO(Y ~ ., data = data.frame(Y=factor(Y),Xfull), control = Weka_control(M = TRUE ) , subset = ((1:1074)[-test.indexes]))
# fit12.predict =predict(fit12, newdata= data.frame(x[test.indexes,]), type="probability" )[,2]
#  fit12.RMSE = sqrt(mean((fit12.predict-y.test)^2))
# Fit 13 = Simple Mean
fit13.predict = mean(y.train)
fit13.RSME= sqrt(mean((fit13.predict-y.test)^2))
#"SVM-SMO",
models = cbind("Lasso", "Elastic Net (a = .5)","Elastic Net (a = .25)", "Bayesian GLM", "BART", "Random Forest", "KRLS",  "Simple Average")
#fit12.RMSE,
RMSE.all = rbind(fit1.logistPred.RMSE,fit2.logistPred.RMSE,fit3.logistPred.RMSE,fit6.logistPred.RMSE,fit8.rmse,fit9.rmse,fit11.rmse, fit13.RSME)
#models,
return(data.frame(RMSE.all))
}
for(i in 1:10){
df$i <-RMSEforModel(Xfull,Y,indexes.matrix[i,])
}
View(df)
df <-RMSEforModel(Xfull,Y,indexes.matrix[1,])
df$Partion2 <-RMSEforModel(Xfull,Y,indexes.matrix[2,])
df$Partion3 <-RMSEforModel(Xfull,Y,indexes.matrix[3,])
df$Partion4 <-RMSEforModel(Xfull,Y,indexes.matrix[4,])
df$Partion5 <-RMSEforModel(Xfull,Y,indexes.matrix[5,])
df$Partion6 <-RMSEforModel(Xfull,Y,indexes.matrix[6,])
df$Partion7 <-RMSEforModel(Xfull,Y,indexes.matrix[7,])
df$Partion8 <-RMSEforModel(Xfull,Y,indexes.matrix[8,])
df$Partion9 <-RMSEforModel(Xfull,Y,indexes.matrix[9,])
df$Partion10 <-RMSEforModel(Xfull,Y,indexes.matrix[10,])
View(covs)
View(df)
df_rotate <- t(df)
t(df)
?t
rotated <- matrix(NA, nrow = 10, ncol =10 )
rotated
rotated[,1] <-df$Partion2
View(df)
covs
Xfull
View(Xfull)
View(Xfull)
=======
fit11.predict = predict(fit11,newdata = x.test[,-1])$fit
fit11.rmse = sqrt(mean((fit11.predict-y.test)^2))
# Fit 12 = SVM-SMO
library(rJava)
.jinit(parameters="-Xmx4g")
library(RWeka)
subset.index = (1:length(y))[-test.indexes]
fit12 <- SMO(y ~ ., data = data.frame(y=factor(y),x), control = Weka_control(M = TRUE ) , subset = subset.index)
fit12.predict =predict(fit12, newdata= data.frame(x[test.indexes,]), type="probability" )[,2]
fit12.RMSE = sqrt(mean((fit12.predict-y.test)^2))
# Fit 13 = Simple Mean
fit13.predict = mean(y.train)
fit13.RSME= sqrt(mean((fit13.predict-y.test)^2))
fit1.logistPred
y[15]
fit1.logistPred[2]
fit1.logistPred[2,]
fit1.logistPred[.2]
fit1.logistPred[,2]
fit1.logistPred
class(fit1.logistPred)
Preds.All = rbind(fit1.logistPred,fit2.logistPred,fit3.logistPred,fit6.predict,fit8.pred,fit9.pred,fit11.predict,fit13.predict)
Preds.All = cbind(fit1.logistPred,fit2.logistPred,fit3.logistPred,fit6.predict,fit8.pred,fit9.pred,fit11.predict,fit13.predict)
Preds.All
class(preds.ALl)
class(preds.All)
class(Preds.All)
fit1.logistPred
fit2.logistPred
fit3.logistPred
fit6.predict
fit8.pred
Pres.All
Preds.All
fit9.pred
fit11.predict
preds = matrix()
preds = rbind(preds,Preds.ALl)
preds = rbind(preds,Preds.All)
?matrix
dim(Preds.All)
fit12.predict
Preds.All = cbind(fit1.logistPred,fit2.logistPred,fit3.logistPred,fit6.predict,fit8.pred,fit9.pred,fit11.predict, fit12.predict, fit13.predict)
Preds.All
models = rbind("Lasso", "Elastic Net (a = .5)","Elastic Net (a = .25)", "Bayesian GLM", "BART", "Random Forest", "KRLS", "SVM_SMO", "Simple Average")
length(models)
#RMSE.all = rbind(fit1.logistPred.RMSE,fit2.logistPred.RMSE,fit3.logistPred.RMSE,fit6.logistPred.RMSE,fit8.rmse,fit9.rmse,fit11.rmse,fit12.RMSE, fit13.RSME)
colnames(Preds.All) = models
Preds.All
preds
#Makes data frame with all 10 RMSE
preds = matrix(data = NA, ncol = 9);
preds
preds = rbind(preds,Preds.All)
preds
preds = rbind(preds,Preds.All)
preds
preds = rbind(preds,Preds.All)
?order
order(preds)
sort(preds)
preds[orders(rownames(preds)),]
preds[order
(rownames(preds)),]
rownames(preds)
j = preds[orders(rownames(preds)),]
j = preds[orderrownames(preds)),]
j = preds[order(rownames(preds)),]
j
j(1:10,)
j[1:10,]
j = preds[order(as.numeric(rownames(preds))),]
j
test.indexes
>>>>>>> 1f7358e7455812cb5d9c5a2eb221c161a9adddd8
