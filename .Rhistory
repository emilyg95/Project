## Improving code
rm(list=ls())
## Chapter 16 in Advanced R provides some philosophical discussion regarding
## the distinction between R as both a language and as an implementation of
## that language. While interesting and helpful, we will focus on some more
## applied work today.
## Three main topics: benchmarking, vectorizing, parallel
## Benchmarking allows you to measure the time that functions take to be executed
## Easy approach: system.time
x <- runif(500)
system.time(sqrt(x))
## The values presented (user, system, and elapsed) will be defined by your
## operating system, but generally, the user time relates to the execution of
## the code, the system time relates to your CPU, and the elapsed time is the
## difference in times since you started the stopwatch (and will be equal to
## the sum of user and system times if the chunk of code was run altogether).
system.time(replicate(100000,sqrt(x)))
## Much better than system.time: microbenchmark
install.packages("microbenchmark")
library(microbenchmark)
?microbenchmark
microbenchmark(sqrt(x)) # evalues 100 times per default
microbenchmark(sqrt(x), times=1000)
## Now we can compare different functions
microbenchmark(sqrt(x),
x^0.5,
times=1000)
microbenchmark(sqrt(x), x^0.5, x^(1/2), exp(log(x)/2),
times=1000)
## And also completely different functions
microbenchmark(sqrt(x), x^4-3*x)
## For ease of interpretation, if a microbenchmark takes
## 1 millisecond, then 1,000 calls take a second
## 1 microsecond, then 1,000,000 calls take a second
## 1 nanosecond,  then 1,000,000,000 calls take a second
## Or use unit=eps for evaluations per second
microbenchmark(sqrt(x), x^0.5, x^(1/2), exp(log(x)/2),
unit="eps")
## Evaluating every function takes time
## Evaluating () or {} takes time
## Even specifying useless arguments in functions takes time!
f0 <- function() NULL
f1 <- function(a=1) NULL
f2 <- function(a=1, b=2) NULL
f3 <- function(a=1, b=2, c=3) NULL
f4 <- function(a=1, b=2, c=3, d=4) NULL
f5 <- function(a=1, b=2, c=3, d=4, e=5) NULL
microbenchmark(f0(), f1(), f2(), f3(), f4(), f5(), times=10000)
## There is room for improvement (or mistakes) in the most basic functions!
## Extracting one element of a data frame
?mtcars
head(mtcars)
microbenchmark(
"[32, 11]" = mtcars[32,11],
"$carb[32]"	= mtcars$carb[32],
"[[c(11, 32)]]" = mtcars[[c(11,32)]],
"[[11]][32]" = mtcars[[11]][32],
".subset2" = .subset2(mtcars,11)[32])
### Class exercise
### Benchmark addsquares for x=4, y=5
### Compare 4^2 + 5^2 to the S3 and S4 methods
microbenchmark(4^2 + 5^2)
## Identifying bad function performance using profiling
## Install Hadley's lineprof
install.packages("profvis")
library(profvis)
f<-function() {
pause(0.1)
g()
h()
}
g<-function() {
pause(0.15)
h()
}
h<-function() {
pause(0.2)
}
prof <-profvis(f())
prof
## Improving function performance
### 1. Use functions tailored to a more specific type of input/output
### This is only a possibility if you have specific knowledge about your
### data and/or the functions you are using
## Examples: vapply() is faster than sapply() because it pre-specifies
## the output type.
## If you want to see if a vector contains a single value,any(x == 10)
## is much faster than 10 %in% x. This is because testing equality is
## simpler than testing inclusion in a set.
## read.csv() will be faster if you specify known column types with
## colClasses.
## see p. 361-364 of Advanced R for an example of how you can modify
## existing base R functions to speed them up for very specific needs
### 2. Vectorizing
### The key idea behind vectorizing your code is to think about entire
### vectors instead of thinking about their components. Using apply and co
### instead of for loops is a start, but does not really solve this issue.
### Truly vectorized functions will make use of code written in C instead
### of R. Loops in C are much faster because they have much less overhead.
## Examples
## Addition on each element of a data frame
rm(list=ls())
m=5
n=5
matrix1 <- replicate(m, rnorm(n)) # create matrix
matdf <- matdf1 <- matdf2 <- data.frame(matrix1) # transform into data frame
matdf
for (i in 1:m) {
for (j in 1:n) {
matdf1[i,j] <- matdf1[i,j] + 1.87*cos(.25)*pi # addition
}
}
matdf1
matdf2 <- matdf2 + 1.87*cos(.25)*pi
matdf2
microbenchmark(
"loop" = for (i in 1:m) {
for (j in 1:n) {
matdf[i,j] <- matdf[i,j] + 1.87*cos(.25)*pi
}
},
"vectorized" = matdf <- matdf + 1.87*cos(.25)*pi
)
## rowSums / colSums
mat1 <- matrix(abs(rnorm(2500))+pi, ncol=50)
head(mat1)[,1:5]
apply(mat1, 1, function(x) sum(x))
rowSums(mat1)
microbenchmark(apply(mat1, 1, function(x) sum(x)),
rowSums(mat1))
## rowMeans/colMeans
apply(mat1, 2, function(x) mean(x))
colMeans(mat1)
microbenchmark(apply(mat1, 2, function(x) mean(x)),
colMeans(mat1))
## Even when working with matrices, think about the actual
## calculations you perform
mat2 <- matrix(sample(1:7, 90000, replace=T), ncol=300)
mat3 <- matrix(sample(2:6, 90000, replace=T), ncol=300)
ys <- sample(3:5, 300, replace=T)
all.equal(mat2 %*% mat3 %*% ys , mat2 %*% (mat3 %*% ys))
microbenchmark(mat2 %*% mat3 %*% ys,
mat2 %*% (mat3 %*% ys))
## Crossproducts
mat4 <- matrix(1:4, ncol=2)
mat5 <- matrix(5:8, ncol=2)
microbenchmark(t(mat4)%*%mat5,
crossprod(mat4, mat5))
## Paste/collapse and copies
random_states <- function() {
paste(sample(state.name,10,replace =TRUE),collapse ="")
}
states10 <- replicate(10, random_states())
states10
states100 <- replicate(100, random_states())
collapse <- function(states) {
out <- ""
for (x in states) {
out <- paste0(out, x) # same as paste(..., sep="", collapse)
}
out
}
microbenchmark(
"loop10" = collapse(states10),
"vec10" = paste(states10, collapse =""),
"loop100" = collapse(states100),
"vec100" = paste(states100, collapse ="")
)
## Here, we are not only getting around using the loop, but also
## avoiding copies. Whenever you append(), cbind(), rbind(), or
## paste() to create a bigger object, R must first allocate space
## for the new object and then copy the old object to its new home.
## If you're repeating this many times, like in a for loop, this
## can be quite computationally expensive.
## Parallelization
## Parallelization uses multiple cores to work simultaneously on different
## parts of a problem. It doesn't reduce the computing time, but it saves
## your time because you're using more of your computer's resources.
install.packages("parallel")
library(parallel)
cores <- detectCores()
cores
pause <- function(i) {
function(x) Sys.sleep(i)
}
## On a Mac:
microbenchmark(
lapply(1:4, pause(0.25)),
mclapply(1:4, pause(0.25), mc.cores = cores),
times=10
)
## On a Windows machine:
cluster <- makePSOCKcluster(cores)
microbenchmark(
parLapply(cluster, 1:4, pause(0.25)),
lapply(1:4, pause(0.25)),
times=10
)
## More generally with apply/plyr family
library(plyr)
bigmat <- matrix(rnorm(90000), ncol=300)
dim(bigmat)
## Mac:
install.packages("doMC")
library(doMC)
registerDoMC(3) # register number of cores
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
times=20
)
## But of course we now know that this should really be colSums
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
"vectorized" = colSums(bigmat),
times=20
)
## Windows:
install.packages("foreach")
library(foreach)
install.packages("doSNOW")
library(doSNOW)
registerDoSNOW(makeCluster(2, type = "SOCK")) # set to two cores
getDoParWorkers() # check number of cores
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
times=20
)
## But of course we now know that this should really be colSums
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
"vectorized" = colSums(bigmat),
times=20
)
install.packages("microbenchmark")
install.packages("doSNOW")
install.packages("foreach")
install.packages("doSNOW")
install.packages("foreach")
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
"vectorized" = colSums(bigmat),
times=20
)
warnings()
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
"vectorized" = colSums(bigmat),
times=20
)
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"vectorized" = colSums(bigmat),
times=20
)
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum),
"vectorized" = colSums(bigmat),
times=20
)
install.packages("foreach")
library(foreach)
install.packages("foreach")
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum),
"vectorized" = colSums(bigmat),
times=20
)
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
"vectorized" = colSums(bigmat),
times=20
)
library(foreach)
library(doSNOW)
registerDoSNOW(makeCluster(2, type = "SOCK")) # set to two cores
getDoParWorkers() # check number of cores
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
times=20
)
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
"vectorized" = colSums(bigmat),
times=20
)
warnings()
library(microbenchmark)
microbenchmark(sqrt(x),
x^0.5,
times=1000)
head(mtcars)
microbenchmark(
"[32, 11]" = mtcars[32,11],
"$carb[32]"	= mtcars$carb[32],
"[[c(11, 32)]]" = mtcars[[c(11,32)]],
"[[11]][32]" = mtcars[[11]][32],
".subset2" = .subset2(mtcars,11)[32])
library(profvis)
m=5
n=5
matrix1 <- replicate(m, rnorm(n)) # create matrix
matdf <- matdf1 <- matdf2 <- data.frame(matrix1) # transform into data frame
matdf
for (i in 1:m) {
for (j in 1:n) {
matdf1[i,j] <- matdf1[i,j] + 1.87*cos(.25)*pi # addition
}
}
matdf1
matdf2 <- matdf2 + 1.87*cos(.25)*pi
matdf2
microbenchmark(
"loop" = for (i in 1:m) {
for (j in 1:n) {
matdf[i,j] <- matdf[i,j] + 1.87*cos(.25)*pi
}
},
"vectorized" = matdf <- matdf + 1.87*cos(.25)*pi
)
mat1 <- matrix(abs(rnorm(2500))+pi, ncol=50)
head(mat1)[,1:5]
apply(mat1, 1, function(x) sum(x))
rowSums(mat1)
microbenchmark(apply(mat1, 1, function(x) sum(x)),
rowSums(mat1))
apply(mat1, 2, function(x) mean(x))
colMeans(mat1)
microbenchmark(apply(mat1, 2, function(x) mean(x)),
colMeans(mat1))
mat2 <- matrix(sample(1:7, 90000, replace=T), ncol=300)
mat3 <- matrix(sample(2:6, 90000, replace=T), ncol=300)
ys <- sample(3:5, 300, replace=T)
all.equal(mat2 %*% mat3 %*% ys , mat2 %*% (mat3 %*% ys))
microbenchmark(mat2 %*% mat3 %*% ys,
mat2 %*% (mat3 %*% ys))
mat4 <- matrix(1:4, ncol=2)
mat5 <- matrix(5:8, ncol=2)
microbenchmark(t(mat4)%*%mat5,
crossprod(mat4, mat5))
random_states <- function() {
paste(sample(state.name,10,replace =TRUE),collapse ="")
}
states10 <- replicate(10, random_states())
states10
states100 <- replicate(100, random_states())
collapse <- function(states) {
out <- ""
for (x in states) {
out <- paste0(out, x) # same as paste(..., sep="", collapse)
}
out
}
microbenchmark(
"loop10" = collapse(states10),
"vec10" = paste(states10, collapse =""),
"loop100" = collapse(states100),
"vec100" = paste(states100, collapse ="")
)
library(parallel)
cores <- detectCores()
cores
pause <- function(i) {
function(x) Sys.sleep(i)
}
cluster <- makePSOCKcluster(cores)
microbenchmark(
parLapply(cluster, 1:4, pause(0.25)),
lapply(1:4, pause(0.25)),
times=10
)
library(plyr)
bigmat <- matrix(rnorm(90000), ncol=300)
dim(bigmat)
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
"vectorized" = colSums(bigmat),
times=20
)
library(foreach)
library(foreach)
library(doSNOW)
registerDoSNOW(makeCluster(2, type = "SOCK")) # set to two cores
getDoParWorkers() # check number of cores
microbenchmark(
"solo" = aaply(bigmat, 2, sum),
"parallel" = aaply(bigmat, 2, sum, .parallel=T),
times=20
)
library(readr)
Multiple_Cause_of_Death_1999_2016 <- read_csv("~/Opioid Public Policy Project/Multiple Cause of Death, 1999-2016.txt")
View(Multiple_Cause_of_Death_1999_2016)
View(Multiple_Cause_of_Death_1999_2016)
setwd("~/GitHub/Project")
load("~/GitHub/Project/CVresultsApril15.RData")
load("~/GitHub/Project/Bootstrap Samples.RData")
regress.func <- function(Y, preds.var){
# need to smartly figure out which columns are not NA
orgcols <- length(preds.var[1,])
notNA <- which(!is.na(preds.var[1,]))
predX <- preds.var[,notNA ]
predX <-predX[1:1074,]#had to het rid of NA's. Also make same number of rows as Y
library(quadprog)
d.mat <- solve(chol(t(predX)%*%predX))
a.mat <- cbind(rep(1, ncol(predX)), diag(ncol(predX)))
b.vec <- c(1, rep(0, ncol(predX)))
d.vec <- t(Y) %*% predX #doesn't like diff number of rows
out<- solve.QP(Dmat = d.mat, factorized =TRUE, dvec = d.vec, Amat = a.mat, bvec = b.vec, meq = 1)
coefs <- rep(NA, orgcols)
notDel <- c(1:orgcols)[notNA]#[notCor]
coefs[notDel] <- out$solution
return(coefs)
}
regress.func(Y, preds.in.order)
regress.func(Y, results[,,1])
regress.func(Y, results[,,2])
for(i in 1:500){
x[,i] <<- regress.func(Y, results[,,i])
}
x<- matrix()
x<- data.frame()
for(i in 1:500){
x[,i] <<- regress.func(Y, results[,,i])
}
x<- data.frame()
x
regress.func(Y, results[,,2])
x<- matrix(nrow = 500, ncol = 9)
x
for(i in 1:500){
x[,i] <<- regress.func(Y, results[,,i])
}
for(i in 1:500){
x[i,] <<- regress.func(Y, results[,,i])
}
for(i in 1:500){
x<- matrix(nrow = 500, ncol = 9)
x[i,] <<- regress.func(Y, results[,,i])
}
final<- matrix(nrow = 500, ncol = 9)
for(i in 1:500){
x<- matrix(nrow = 500, ncol = 9)
x[i,] <- regress.func(Y, results[,,i])
}
for(i in 1:500){
x<- matrix(nrow = 500, ncol = 9)
x[i,] <- regress.func(Y, results[,,i])
return(x)
}
View(x)
for(i in 1:500){
x<- matrix(nrow = 500, ncol = 9)
x[,i] <- regress.func(Y, results[,,i])
return(x)
}
View(x)
for(i in 1:500){
final[,i] <- regress.func(Y, results[,,i])
}
final<- matrix(nrow = 500, ncol = 9)
for(i in 1:500){
final[i,] <- regress.func(Y, results[,,i])
}
View(final)
regress.func.results<- matrix(nrow = 500, ncol = 9)
for(i in 1:500){
regress.func.results[i,] <- regress.func(Y, results[,,i])
}
View(regress.func.results)
regress.func(Y, results[,,1])
View(regress.func.results)
install.packages("EBMAforecast")
library(EBMAforecast)
ForecastData = makeForecastData(.predCalibration = preds.in.order, .outcomeCalibration = Y, .modelNames = Names)
Names = c("Lasso", "Elastic Net a = .5", "Elastic Net a = .25", "Bayesian GLM", "BART", "Random Forest", "KRLS", "SVM_SMO", "Simple Average")
ForecastData = makeForecastData(.predCalibration = preds.in.order, .outcomeCalibration = Y, .modelNames = Names)
myCal<-calibrateEnsemble(ForecastData)
ForecastData = makeForecastData(.predCalibration = preds.in.order, .outcomeCalibration = Y, .modelNames = Names)
Names = c("Lasso", "Elastic Net a = .5", "Elastic Net a = .25", "Bayesian GLM", "BART", "Random Forest", "KRLS", "SVM_SMO", "Simple Average")
ForecastData = makeForecastData(.predCalibration = preds.in.order, .outcomeCalibration = Y, .modelNames = Names)
myCal<-calibrateEnsemble(ForecastData)
myCal<-calibrateEnsemble(ForecastData)
myCal@modelWeights
ForecastData = makeForecastData(.predCalibration = preds.in.order, .outcomeCalibration = Y, .modelNames = Names)
myCal<-calibrateEnsemble(ForecastData)
myCal@modelWeights
View(preds.in.order)
library(EBMAforecast)
Names = c("Lasso", "Elastic Net a = .5", "Elastic Net a = .25", "Bayesian GLM", "BART", "Random Forest", "KRLS", "SVM_SMO", "Simple Average")
ForecastData = makeForecastData(.predCalibration = preds.in.order, .outcomeCalibration = Y, .modelNames = Names)
myCal<-calibrateEnsemble(ForecastData)
regress.func(Y, preds.in.order)
install.packages("EBMAforecast")
install.packages("EBMAforecast")
install.packages("EBMAforecast")
install.packages("EBMAforecast")
install.packages("EBMAforecast")
library(EBMAforecast)
Names = c("Lasso", "Elastic Net a = .5", "Elastic Net a = .25", "Bayesian GLM", "BART", "Random Forest", "KRLS", "SVM_SMO", "Simple Average")
ForecastData = makeForecastData(.predCalibration = preds.in.order, .outcomeCalibration = Y, .modelNames = Names)
myCal<-calibrateEnsemble(ForecastData)
